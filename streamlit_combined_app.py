import os
import streamlit as st
import time
import base64
import re
from dotenv import load_dotenv
from langchain_groq import ChatGroq
from backend.cv_ingest import extract_cv_content, detect_language
from langchain_core.messages import SystemMessage, HumanMessage

# Load environment variables
load_dotenv()

# Initialize Groq client
groq_api_key = os.getenv("GROQ_API_KEY")
if not groq_api_key:
    st.error("GROQ_API_KEY environment variable is not set")
    st.stop()

# Load CV content at startup
CV_PATH = os.path.join(os.path.dirname(__file__), "cv.docx")
CV_CONTENT = extract_cv_content(CV_PATH)

if not CV_CONTENT:
    st.error("Failed to load CV content. Please check the file path.")
    st.stop()

# Initialize the ChatGroq client
def get_groq_client(model_name="deepseek-r1-distill-llama-70b", temperature=0.7):
    return ChatGroq(
        model_name=model_name,
        temperature=temperature,
    )

# Function to process chat messages
def process_message(message, model="deepseek-r1-distill-llama-70b", temperature=0.7):
    start_time = time.time()
    
    # Detect language of the incoming message
    lang = detect_language(message)
    
    # Prepare system message based on language
    if lang == 'ar':
        system_message = """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…ÙÙŠØ¯ ÙˆÙ…Ø­ØªØ±Ù. Ø£Ù†Øª ØªÙ…Ø«Ù„ ØµØ§Ø­Ø¨ Ø§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ø°Ø§ØªÙŠØ© ÙˆØªØ¬ÙŠØ¨ Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø­ÙˆÙ„ Ø®Ø¨Ø±Ø§ØªÙƒ ÙˆÙ…Ù‡Ø§Ø±Ø§ØªÙƒ ÙˆÙ…Ø¤Ù‡Ù„Ø§ØªÙƒ.
        ÙŠØ¬Ø¨ Ø£Ù† ØªØ³ØªÙ…Ø¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª ÙÙ‚Ø· Ù…Ù† Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ø°Ø§ØªÙŠØ© Ø§Ù„Ù…Ø±ÙÙ‚Ø©. Ø¥Ø°Ø§ Ø³ÙØ¦Ù„Øª Ø¹Ù† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ø°Ø§ØªÙŠØ©ØŒ ÙÙ‚Ù„ Ø¨Ø£Ø¯Ø¨ Ø¥Ù†Ùƒ Ù„Ø§ ØªÙ…Ù„Ùƒ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª. ÙˆØ§Ø¬Ø¹Ù„Ù‡ ÙŠØªÙˆØ§ØµÙ„ Ù…Ø¹Ù‰ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ù‡ Ø¨Ø§Ù„ CV
        Ø§Ø­ØªÙØ¸ Ø¨Ø§Ù„Ø±Ø¯ÙˆØ¯ Ù…ÙˆØ¬Ø²Ø© ÙˆÙ…Ù‡Ù†ÙŠØ© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø©. Ø­Ø§ÙˆÙ„ Ø£Ù† ØªÙƒÙˆÙ† Ù…ÙÙŠØ¯Ù‹Ø§ Ù‚Ø¯Ø± Ø§Ù„Ø¥Ù…ÙƒØ§Ù† Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ø°ÙŠÙ† ÙŠØ³ØªÙØ³Ø±ÙˆÙ† Ø¹Ù† Ù…Ø¤Ù‡Ù„Ø§ØªÙƒ.
        
        Ù‚Ù… Ø¨ØªØ¶Ù…ÙŠÙ† ØªÙÙƒÙŠØ±Ùƒ ÙÙŠ Ø¹Ù„Ø§Ù…Ø§Øª <think></think> Ù‚Ø¨Ù„ Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©. Ø³ÙŠØªÙ… Ø¹Ø±Ø¶ Ù‡Ø°Ø§ Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ Ù‚Ø³Ù… Ù‚Ø§Ø¨Ù„ Ù„Ù„Ø·ÙŠ ÙÙŠ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…."""
    else:  # Default to English
        system_message = """You are a helpful and professional assistant. You are representing the CV owner and answering users' questions about your experience, skills, and qualifications.
        All answers must be drawn only from the content of the uploaded CV. If asked about information not in the CV, politely state that you don't have that information, And let him contact me through the means of communication available in the CV.
        Keep replies concise, professional, and on-point. Try to be as helpful as possible to users inquiring about your qualifications.
        
        Include your thinking process within <think></think> tags before your final answer. This thinking will be shown in a collapsible section in the UI."""
    
    try:
        # Initialize Groq client with requested model and temperature
        groq_client = get_groq_client(model_name=model, temperature=temperature)
        
        # Convert messages to the format expected by langchain
        langchain_messages = [
            SystemMessage(content=system_message),
            SystemMessage(content=f"CV Content: {CV_CONTENT}"),
            HumanMessage(content=message)
        ]
        
        # Get response from Groq
        response = groq_client.invoke(langchain_messages)
        
        # Extract the content from the response
        reply = response.content
        
        # Calculate processing time
        processing_time = time.time() - start_time
        
        return reply, processing_time
    except Exception as e:
        processing_time = time.time() - start_time
        return f"Error generating response: {str(e)}", processing_time

# Function to convert image to base64
def get_image_as_base64(image_path):
    with open(image_path, "rb") as img_file:
        return base64.b64encode(img_file.read()).decode()

# Configure Streamlit page
st.set_page_config(
    page_title="CV Chatbot",
    page_icon="ğŸ’¼",
    layout="centered"
)

# Custom CSS for styling
st.markdown("""
<style>
    .main {padding: 2rem;}
    .stTextInput {margin-bottom: 0.5rem;}
    
    /* Remove default Streamlit elements */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    .stDeployButton {display:none;}
    
    /* Header styling */
    .header {
        display: flex;
        align-items: center;
        margin-bottom: 1.5rem;
        padding: 1rem;
        background-color: #f8f9fa;
        border-radius: 0.5rem;
    }
    .header img {
        width: 80px;
        height: 80px;
        border-radius: 50%;
        margin-right: 1rem;
        object-fit: cover;
    }
    .header-text h1 {
        margin: 0;
        color: #1e3a8a;
    }
    .header-text p {
        margin: 0;
        color: #4b5563;
    }
    
    /* Expander styling */
    .streamlit-expanderHeader {
        font-size: 1rem;
        color: #4b5563;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state for chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display header with profile picture
image_path = os.path.join(os.path.dirname(__file__), "frontend", "image.png")

# Header with profile image
st.markdown(
    f"""
    <div class="header">
        <img src="data:image/png;base64,{get_image_as_base64(image_path)}" alt="Profile Picture">
        <div class="header-text">
            <h1>Letâ€™s chat with me! ğŸ§ </h1>
            <p>Ask me anything about my professional experience</p>
        </div>
    </div>
    """,
    unsafe_allow_html=True
)

# Display chat messages using Streamlit's built-in chat elements
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Chat input using Streamlit's chat_input
if prompt := st.chat_input("Ask me anything..."):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # Display user message
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Get chatbot response
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response, processing_time = process_message(prompt)
            
            # Parse response for <think> tags
            pattern = r"<think>(.*?)</think>(.*)"
            match = re.search(pattern, response, re.DOTALL)
            
            if match:
                think_content = match.group(1).strip()
                output_content = match.group(2).strip()
                
                # Display thinking in collapsible section
                with st.expander("ğŸ¤” View thinking process"):
                    st.markdown(think_content)
                
                # Display final answer
                st.markdown(output_content)
                
                # Add only the output content to chat history
                st.session_state.messages.append({"role": "assistant", "content": output_content})
            else:
                # If no thinking tags, display the whole response
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})